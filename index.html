<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="BRICS: Bi-level Feature Representation of Image CollectionS">
  <meta name="keywords" content="Representation Learning, Generative Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BRICS: Bi-level Feature Representation of Image CollectionS</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!--
  <link rel="icon" href="./static/images/favicon.svg">
  -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>
-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">BRICS: Bi-level Feature Representation of Image CollectionS</h1>
            <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/brics-project/brics_code_release"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/concept_of_our_method.png"
           alt="concept of our method"
      >
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Brics</span> projects images into <b>key codes</b>  and then
        uses the key codes to retrieve features from multi-scale <b>feature grids</b>, instead of directly encoding images into features.</h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <img src="static/images/ffhq_web_show_01.png">
        </div>
        <div class="item item-chair-tp">
          <img src="static/images/ffhq_web_show_02.png">
        </div>
        <div class="item item-shiba">
          <img src="static/images/ffhq_web_show_03.png">
        </div>
        <div class="item item-fullbody">
          <img src="static/images/church_web_show_01.png">
        </div>
        <div class="item item-blueshirt">
          <img src="static/images/church_web_show_02.png">
        </div>
        <div class="item item-mask">
          <img src="static/images/church_web_show_03.png">
        </div>
      </div>
    </div>
      <h2 class="subtitle has-text-centered">
        <u>Uncurated</u> generated results from the diffusion model trained on our <b>key codes</b>.
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          We present BRICS, a <em>bi-level</em> feature representation for image collections,
consisting of a key code space on top of a multi-scale feature grid space.
Our representation is learned by an autoencoder to encode images into <em>continuous</em>
key codes, which are used to retrieve features from groups of multi-resolution feature
grids. Our key codes and feature grids are jointly trained continuously with well-defined gradient flows, leading to high usage of the feature grids and improved generative modeling compared to discrete Vector Quantization (VQ).
Differently from existing continuous representations such as KL-regularized latent codes, our key codes are strictly bounded in 
scale and variance. Overall, feature encoding by BRICS is compact, efficient to train, and enables generative modeling over key codes using the diffusion model. Experimental results show that our method achieves comparable reconstruction results to VQ methods while having a smaller and more efficient decoder network (<b>≈50% fewer</b> GFlops). By applying the diffusion model over our key code space, we achieve
state-of-the-art performance on image synthesis on the LSUN-Church (<b>≈26% lower</b> than <a href="https://arxiv.org/abs/2112.10752">LDM</a>, <b>≈29% lower</b> than <a href="https://arxiv.org/abs/1912.04958">StyleGAN2</a> and <b>≈42% lower</b> than <a href="https://arxiv.org/abs/2111.01007">Projected GAN</a> on CLIP-FID) and FFHQ datasets. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. --
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
        </div>
      </div>
    </div>
    --/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Pipeline </h2>
          <p>
            Overall pipeline of our method in three parts: Encoding, Feature Retrieval and Decoding.
          </p>
          <img src="static/images/pipeline.png">
        </div>
      </div>

      <!--
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
      -->
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Reconstruction</h3>
        <div class="content has-text-justified">
          <p>
            Our largest model (code size 16 × 16 × 16) has only 10M
more parameters (≈ 24% increase) than those of the corresponding VQGAN and RQ-VAE models.
However, our model outperforms VQGAN and RQ-VAE across all metrics, showing <b>55% and 41%
improvements</b> in LPIPS, respectively. In terms of computational costs, our method requires <b>≈ 50%
fewer GFlops </b>due to its smaller decoder and the efficiency gained from utilizing the multi-scale feature grids.
          </p>
        </div>

        <div class="column">
          <div class="content">
            <img src="static/images/recon_results.png">
            <p align="center">
              Reconstruction metrics on the validation splits of FFHQ and LSUN-Church dataset.
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content">
            <img src="static/images/size_of_models.png">
            <p align="center">
              Trainable parameters and computational load of decoders.
              An <sup>*</sup> indicates total number of parameters in feature grids and
              <sup>&Dagger;</sup> refers to total computational cost of decoding <em>and</em>
              feature retrieval from feature grids.
            </p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Generation</h3>
        <div class="content has-text-justified">
          <p>
            We achieve the state-of-the-art <b>CLIP-FID</b> than others (<b>≈42% lower</b> than Projected GAN, <b>≈29% lower</b> than StyleGAN2, and <b>≈26% lower</b> than LDM) while maintaining competitive on all other metrics. Moreover, the <b>precision </b> score of our
generated images is significantly higher than others, indicating a substantial reduction of low-quality
samples in our results, while our recall is the second best in LSUN-Church and is almost the same as StyleGAN2 in FFHQ dataset.
          </p>
        </div>
        <div class="column">
          <div class="content">
            <img src="static/images/church_gen_quant_results.png" class="center_table">
            <p align="center">
              Quantitative results of generation on LSUN-Church dataset. 
              Our relaxed precision (Config d) method is adapted to two distinct noise schedulers(Config a,b) and 
              gets record low CLIP-FID in Config b + d, while Config a + c (KL-reg with min-snr noise scheduler) gets much worse results. * denotes that we measure the metrics of Projected GAN 
              using the checkpoint provided from the <a href="https://github.com/autonomousvision/projected-gan">official Projected GAN Github repository</a>. Underlined numbers are the second best results.
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content">
            <img src="static/images/ffhq_gen_quant_results.png" class="center_table">
            <p align="center">
              Quantitative results of generation on FFHQ dataset. <sup>*</sup> denotes the results calculated 
              on publicly released checkpoint by LDM author on Github.
            </p>
          </div>
        </div>
        <!--/ Re-rendering. -->

        <!-- Nearest Neighbour -->
        <h3 class="title is-4">Nearest Neighbour</h3>
        <div class="content has-text-justified">
          <p>
            Although our method has much higher precision scores than previous methods, we show the nearest neighbour
            search by LPIPS to demonstrate that our generated samples are unique and <b>not</b> mere retrievals from the training dataset.
            In the following images, the <b>leftmost</b> images in each row are generated images from our method and the rest images in each row
            are the nearest neighbour search results.
          </p>
        </div>
        <div class="column">
          <div class="content_hscroll">
            <img src="static/images/ffhq_nearest_sup_show.png">
          </div>
        </div>
        <div class="column">
          <div class="content_hscroll">
            <img src="static/images/lsun_nearest_sup_show.png">
          </div>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->



  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
